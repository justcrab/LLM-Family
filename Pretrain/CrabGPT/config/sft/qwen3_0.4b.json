{
  "dataset": {
    "file": "/public/MountData/xcx/sft/all_sft_data.json"
  },
  "collator" : {
    "max_seq_length": 1024
  },

  "lora": {
    "train_mode": "lora",
    "lora_rank": 64,
    "lora_alpha": 128,
    "lora_dropout": 0.05
  },

  "trainer": {
    "output_dir": "/public/xcx/Item/Pre-Train/CrabGPT/outputs/sft/all_sft_data",
    "num_train_epochs": 1,
    "per_device_train_batch_size": 8,
    "per_device_eval_batch_size": 8,
    "warmup_steps": 200,
    "weight_decay": 0,
    "logging_dir": "./logs/sft",
    "logging_steps": 5,
    "save_steps": 100,
    "evaluation_strategy": "steps",
    "eval_steps": 100,
    "save_total_limit": 2,
    "gradient_accumulation_steps": 4,
    "seed": 42,
    "learning_rate": 3e-5,
    "lr_scheduler_type": "constant_with_warmup",
    "optim": "adamw_torch",
    "bf16": true,
    "adam_beta1": 0.9,
    "adam_beta2": 0.95,
    "ddp_find_unused_parameters": false,
    "gradient_checkpointing": true,
    "report_to": "tensorboard"
  }
}