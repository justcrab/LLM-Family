{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "1 给Qwen1.5添加\\<image\\> token",
   "id": "fd0cada3c786755e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "from transformers import Qwen2TokenizerFast, AutoModelForCausalLM",
   "id": "736c45985389b98a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "llm_path = \"model/qwen1.5_0.5b\"\n",
    "llm_tokenizer = Qwen2TokenizerFast.from_pretrained(llm_path, device_map=\"cuda:0\", local_files_only=True)\n",
    "llm_tokenizer.encode(\"<image>\")"
   ],
   "id": "fa39791c5fe9b8f2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "qwen_model = AutoModelForCausalLM.from_pretrained(llm_path, device_map=\"cuda:0\")\n",
    "print(qwen_model.model)\n",
    "print(qwen_model.model.embed_tokens)"
   ],
   "id": "9d342f85bd9dbd47",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "2 初始化Model",
   "id": "8a1fa3e698d68eb7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-12T07:59:39.219080Z",
     "start_time": "2024-07-12T07:59:31.097789Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoProcessor, AutoTokenizer, AutoModel, AutoModelForCausalLM\n",
    "\n",
    "clip_path, llm_path = \"model/clip\" ,\"model/qwen1.5_0.5b\"\n",
    "clip_processor = AutoProcessor.from_pretrained(clip_path, device_map=\"cuda:0\")\n",
    "llm_tokenizer = AutoTokenizer.from_pretrained(llm_path, device_map=\"cuda:0\")\n",
    "clip_model = AutoModel.from_pretrained(clip_path, device_map=\"cuda:0\")\n",
    "llm_model = AutoModelForCausalLM.from_pretrained(llm_path, device_map=\"cuda:0\")\n",
    "clip_config = clip_model.vision_model.config\n",
    "llm_config = llm_model.config"
   ],
   "id": "1bfff61c7e82f9ea",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "clip_model",
   "id": "60a06c8b495c66b4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "llm_model",
   "id": "1586aa978a5076aa",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-12T07:59:55.255671Z",
     "start_time": "2024-07-12T07:59:41.642419Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import LlavaProcessor, LlavaForConditionalGeneration, LlavaConfig\n",
    "llava_config = LlavaConfig(vision_config=clip_config, text_config=llm_config)\n",
    "llava_model = LlavaForConditionalGeneration(llava_config)"
   ],
   "id": "ed43757458e2d033",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "llava_model",
   "id": "9f6c8010a7d413ad",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "3 替换llava model的视觉和文本模型",
   "id": "b48a70a753664dc8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-12T08:00:01.038843Z",
     "start_time": "2024-07-12T08:00:00.995113Z"
    }
   },
   "cell_type": "code",
   "source": [
    "llava_model.vision_tower.vision_model = clip_model.vision_model\n",
    "llava_model.language_model = llm_model"
   ],
   "id": "11394421fc0eea15",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "4 替换pad_token_id和image_token_index",
   "id": "18ca5554115f0030"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-12T08:00:03.166485Z",
     "start_time": "2024-07-12T08:00:03.155727Z"
    }
   },
   "cell_type": "code",
   "source": [
    "llava_model.config.pad_token_id = llm_tokenizer.pad_token_id\n",
    "llava_model.config.image_token_index = llm_tokenizer.encode(\"<image>\")[0]"
   ],
   "id": "2dc68af3b5798dcd",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "5 保存模型",
   "id": "6fa5a57405a5cb01"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-12T08:00:10.104283Z",
     "start_time": "2024-07-12T08:00:05.360272Z"
    }
   },
   "cell_type": "code",
   "source": [
    "llava_path = \"model/llava\"\n",
    "llava_model.save_pretrained(llava_path)\n",
    "llm_tokenizer.save_pretrained(llava_path)"
   ],
   "id": "5314c79a8e23164d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('model/llava\\\\tokenizer_config.json',\n",
       " 'model/llava\\\\special_tokens_map.json',\n",
       " 'model/llava\\\\vocab.json',\n",
       " 'model/llava\\\\merges.txt',\n",
       " 'model/llava\\\\added_tokens.json',\n",
       " 'model/llava\\\\tokenizer.json')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "6 添加clip的processor到llava模型中\n",
    "将clip的preprocessor_config.json复制粘贴到llava文件夹下"
   ],
   "id": "b0c27d4a670cccaf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "clip_processor.save_pretrained(\"model/processor\")",
   "id": "4cc855f1edfce65b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "7 尝试体验和加载模型",
   "id": "1dec7a1b08dc432d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-20T05:35:13.836353Z",
     "start_time": "2024-07-20T05:35:13.223668Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from PIL import Image\n",
    "from transformers import LlavaProcessor, LlavaForConditionalGeneration\n",
    "\n",
    "url = \"images/001.jpg\"\n",
    "image = Image.open(fp=url)\n",
    "\n",
    "prompt_text = \"<image>\\nWhat are there?\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "    {\"role\": \"user\", \"content\": prompt_text}\n",
    "]\n",
    "\n",
    "llava_path = \"model/llava\"\n",
    "llava_processor = LlavaProcessor.from_pretrained(llava_path, device_map=\"cuda:0\")\n",
    "prompt = llava_processor.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_propmt=True)\n",
    "inputs = llava_processor(text=prompt, images=image)\n",
    "llava_model = LlavaForConditionalGeneration.from_pretrained(llava_path, device_map=\"cuda:0\")"
   ],
   "id": "4f705f536286685b",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[-0.15723877, -0.12804192, -0.11344349, ..., -0.15723877,\n",
       "          0.8354542 ,  1.2004149 ],\n",
       "        [-0.18643562, -0.12804192, -0.15723877, ...,  0.32450938,\n",
       "          1.1274228 ,  1.1858164 ],\n",
       "        [-0.18643562, -0.08424664, -0.06964822, ...,  0.7332653 ,\n",
       "          1.1566195 ,  1.1274228 ],\n",
       "        ...,\n",
       "        [ 0.03254076,  0.00334391, -0.02585294, ...,  0.0617376 ,\n",
       "          0.14932826,  0.20772196],\n",
       "        [ 0.0617376 ,  0.00334391, -0.0550498 , ...,  0.12013142,\n",
       "          0.19312355,  0.23691882],\n",
       "        [ 0.0617376 ,  0.00334391, -0.06964822, ...,  0.17852512,\n",
       "          0.2223204 ,  0.16392669]],\n",
       "\n",
       "       [[-0.08623484, -0.05621931, -0.04121154, ..., -0.2813358 ,\n",
       "          0.7391925 ,  1.1894255 ],\n",
       "        [-0.11625037, -0.05621931, -0.08623484, ...,  0.21392061,\n",
       "          1.0693634 ,  1.1744177 ],\n",
       "        [-0.11625037, -0.011196  ,  0.00381176, ...,  0.66415364,\n",
       "          1.1293944 ,  1.1444021 ],\n",
       "        ...,\n",
       "        [ 0.06384283,  0.0338273 ,  0.0338273 , ...,  0.01881953,\n",
       "          0.10886613,  0.16889732],\n",
       "        [ 0.09385836,  0.0338273 , -0.011196  , ...,  0.0788506 ,\n",
       "          0.15388943,  0.19891284],\n",
       "        [ 0.09385836,  0.0338273 , -0.02620377, ...,  0.13888167,\n",
       "          0.18390508,  0.1238739 ]],\n",
       "\n",
       "       [[-0.32839438, -0.29995427, -0.2857342 , ..., -0.243074  ,\n",
       "          0.6954504 ,  1.0651721 ],\n",
       "        [-0.35683453, -0.29995427, -0.32839438, ...,  0.22618815,\n",
       "          1.022512  ,  1.0793922 ],\n",
       "        [-0.35683453, -0.25729406, -0.243074  , ...,  0.63857013,\n",
       "          1.0793922 ,  1.036732  ],\n",
       "        ...,\n",
       "        [-0.25729406, -0.2857342 , -0.32839438, ..., -0.243074  ,\n",
       "         -0.15775362, -0.10087335],\n",
       "        [-0.22885394, -0.2857342 , -0.32839438, ..., -0.18619375,\n",
       "         -0.11509342, -0.07243322],\n",
       "        [-0.22885394, -0.2857342 , -0.34261447, ..., -0.12931348,\n",
       "         -0.08665328, -0.14353354]]], dtype=float32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-12T08:25:39.982802Z",
     "start_time": "2024-07-12T08:25:37.886536Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for tk in inputs.keys():\n",
    "    inputs[tk] = inputs[tk].to(llava_model.device)\n",
    "generate_ids = llava_model.generate(**inputs, max_new_tokens=20)\n",
    "gen_text = llava_processor.batch_decode(\n",
    "    generate_ids, skip_special_tokens=False, clean_up_tokenization_spaces=False\n",
    ")[0]\n",
    "\n",
    "print(gen_text)"
   ],
   "id": "423a99fc7ac74d21",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are a helpful assistant<|im_end|>\n",
      "<|im_start|>user\n",
      "<image>\n",
      "What are there?<|im_end|>\n",
      "<|im_start|>There are many different types of \"news\" that can be written in the English language. Some\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-14T07:25:02.846635Z",
     "start_time": "2024-07-14T07:24:51.107301Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import LlavaProcessor, LlavaForConditionalGeneration\n",
    "\n",
    "llava_path = \"model/llava\"\n",
    "llava_model = LlavaForConditionalGeneration.from_pretrained(llava_path, device_map=\"cuda:0\")\n",
    "# llava_model.vision_tower.vision_model.requires_grad_=False\n",
    "# llava_model.language_model.requires_grad_=False\n",
    "# llava_model.multi_modal_projector.requires_grad_ = True\n"
   ],
   "id": "c7105c45eddee9c9",
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'LlavaForConditionalGeneration' object has no attribute 'requires_grad'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-1-2d7e0faf5fe0>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      6\u001B[0m \u001B[1;31m# llava_model.language_model.requires_grad_=False\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      7\u001B[0m \u001B[1;31m# llava_model.multi_modal_projector.requires_grad_ = True\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 8\u001B[1;33m \u001B[0mllava_model\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mrequires_grad\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;32mD:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001B[0m in \u001B[0;36m__getattr__\u001B[1;34m(self, name)\u001B[0m\n\u001B[0;32m   1693\u001B[0m             \u001B[1;32mif\u001B[0m \u001B[0mname\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mmodules\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1694\u001B[0m                 \u001B[1;32mreturn\u001B[0m \u001B[0mmodules\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mname\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1695\u001B[1;33m         \u001B[1;32mraise\u001B[0m \u001B[0mAttributeError\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34mf\"'{type(self).__name__}' object has no attribute '{name}'\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1696\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1697\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0m__setattr__\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mname\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mstr\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mvalue\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mUnion\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mTensor\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m'Module'\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m->\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mAttributeError\u001B[0m: 'LlavaForConditionalGeneration' object has no attribute 'requires_grad'"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-14T07:25:35.240495Z",
     "start_time": "2024-07-14T07:25:35.218071Z"
    }
   },
   "cell_type": "code",
   "source": "llava_model",
   "id": "9763110edb822aaa",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlavaForConditionalGeneration(\n",
       "  (vision_tower): CLIPVisionModel(\n",
       "    (vision_model): CLIPVisionTransformer(\n",
       "      (embeddings): CLIPVisionEmbeddings(\n",
       "        (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
       "        (position_embedding): Embedding(577, 1024)\n",
       "      )\n",
       "      (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (encoder): CLIPEncoder(\n",
       "        (layers): ModuleList(\n",
       "          (0-23): 24 x CLIPEncoderLayer(\n",
       "            (self_attn): CLIPAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): CLIPMLP(\n",
       "              (activation_fn): QuickGELUActivation()\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            )\n",
       "            (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (multi_modal_projector): LlavaMultiModalProjector(\n",
       "    (linear_1): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    (act): GELUActivation()\n",
       "    (linear_2): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "  )\n",
       "  (language_model): Qwen2ForCausalLM(\n",
       "    (model): Qwen2Model(\n",
       "      (embed_tokens): Embedding(151936, 1024)\n",
       "      (layers): ModuleList(\n",
       "        (0-23): 24 x Qwen2DecoderLayer(\n",
       "          (self_attn): Qwen2SdpaAttention(\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (o_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (rotary_emb): Qwen2RotaryEmbedding()\n",
       "          )\n",
       "          (mlp): Qwen2MLP(\n",
       "            (gate_proj): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "            (up_proj): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "            (down_proj): Linear(in_features=2816, out_features=1024, bias=False)\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "          (input_layernorm): Qwen2RMSNorm()\n",
       "          (post_attention_layernorm): Qwen2RMSNorm()\n",
       "        )\n",
       "      )\n",
       "      (norm): Qwen2RMSNorm()\n",
       "    )\n",
       "    (lm_head): Linear(in_features=1024, out_features=151936, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-14T07:27:48.110533Z",
     "start_time": "2024-07-14T07:27:48.053582Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for name, parameters in llava_model.named_parameters():\n",
    "    if parameters.requires_grad:\n",
    "        print(name)"
   ],
   "id": "a49575ec9625e65",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vision_tower.vision_model.embeddings.class_embedding\n",
      "vision_tower.vision_model.embeddings.patch_embedding.weight\n",
      "vision_tower.vision_model.embeddings.position_embedding.weight\n",
      "vision_tower.vision_model.pre_layrnorm.weight\n",
      "vision_tower.vision_model.pre_layrnorm.bias\n",
      "vision_tower.vision_model.encoder.layers.0.self_attn.k_proj.weight\n",
      "vision_tower.vision_model.encoder.layers.0.self_attn.k_proj.bias\n",
      "vision_tower.vision_model.encoder.layers.0.self_attn.v_proj.weight\n",
      "vision_tower.vision_model.encoder.layers.0.self_attn.v_proj.bias\n",
      "vision_tower.vision_model.encoder.layers.0.self_attn.q_proj.weight\n",
      "vision_tower.vision_model.encoder.layers.0.self_attn.q_proj.bias\n",
      "vision_tower.vision_model.encoder.layers.0.self_attn.out_proj.weight\n",
      "vision_tower.vision_model.encoder.layers.0.self_attn.out_proj.bias\n",
      "vision_tower.vision_model.encoder.layers.0.layer_norm1.weight\n",
      "vision_tower.vision_model.encoder.layers.0.layer_norm1.bias\n",
      "vision_tower.vision_model.encoder.layers.0.mlp.fc1.weight\n",
      "vision_tower.vision_model.encoder.layers.0.mlp.fc1.bias\n",
      "vision_tower.vision_model.encoder.layers.0.mlp.fc2.weight\n",
      "vision_tower.vision_model.encoder.layers.0.mlp.fc2.bias\n",
      "vision_tower.vision_model.encoder.layers.0.layer_norm2.weight\n",
      "vision_tower.vision_model.encoder.layers.0.layer_norm2.bias\n",
      "vision_tower.vision_model.encoder.layers.1.self_attn.k_proj.weight\n",
      "vision_tower.vision_model.encoder.layers.1.self_attn.k_proj.bias\n",
      "vision_tower.vision_model.encoder.layers.1.self_attn.v_proj.weight\n",
      "vision_tower.vision_model.encoder.layers.1.self_attn.v_proj.bias\n",
      "vision_tower.vision_model.encoder.layers.1.self_attn.q_proj.weight\n",
      "vision_tower.vision_model.encoder.layers.1.self_attn.q_proj.bias\n",
      "vision_tower.vision_model.encoder.layers.1.self_attn.out_proj.weight\n",
      "vision_tower.vision_model.encoder.layers.1.self_attn.out_proj.bias\n",
      "vision_tower.vision_model.encoder.layers.1.layer_norm1.weight\n",
      "vision_tower.vision_model.encoder.layers.1.layer_norm1.bias\n",
      "vision_tower.vision_model.encoder.layers.1.mlp.fc1.weight\n",
      "vision_tower.vision_model.encoder.layers.1.mlp.fc1.bias\n",
      "vision_tower.vision_model.encoder.layers.1.mlp.fc2.weight\n",
      "vision_tower.vision_model.encoder.layers.1.mlp.fc2.bias\n",
      "vision_tower.vision_model.encoder.layers.1.layer_norm2.weight\n",
      "vision_tower.vision_model.encoder.layers.1.layer_norm2.bias\n",
      "vision_tower.vision_model.encoder.layers.2.self_attn.k_proj.weight\n",
      "vision_tower.vision_model.encoder.layers.2.self_attn.k_proj.bias\n",
      "vision_tower.vision_model.encoder.layers.2.self_attn.v_proj.weight\n",
      "vision_tower.vision_model.encoder.layers.2.self_attn.v_proj.bias\n",
      "vision_tower.vision_model.encoder.layers.2.self_attn.q_proj.weight\n",
      "vision_tower.vision_model.encoder.layers.2.self_attn.q_proj.bias\n",
      "vision_tower.vision_model.encoder.layers.2.self_attn.out_proj.weight\n",
      "vision_tower.vision_model.encoder.layers.2.self_attn.out_proj.bias\n",
      "vision_tower.vision_model.encoder.layers.2.layer_norm1.weight\n",
      "vision_tower.vision_model.encoder.layers.2.layer_norm1.bias\n",
      "vision_tower.vision_model.encoder.layers.2.mlp.fc1.weight\n",
      "vision_tower.vision_model.encoder.layers.2.mlp.fc1.bias\n",
      "vision_tower.vision_model.encoder.layers.2.mlp.fc2.weight\n",
      "vision_tower.vision_model.encoder.layers.2.mlp.fc2.bias\n",
      "vision_tower.vision_model.encoder.layers.2.layer_norm2.weight\n",
      "vision_tower.vision_model.encoder.layers.2.layer_norm2.bias\n",
      "vision_tower.vision_model.encoder.layers.3.self_attn.k_proj.weight\n",
      "vision_tower.vision_model.encoder.layers.3.self_attn.k_proj.bias\n",
      "vision_tower.vision_model.encoder.layers.3.self_attn.v_proj.weight\n",
      "vision_tower.vision_model.encoder.layers.3.self_attn.v_proj.bias\n",
      "vision_tower.vision_model.encoder.layers.3.self_attn.q_proj.weight\n",
      "vision_tower.vision_model.encoder.layers.3.self_attn.q_proj.bias\n",
      "vision_tower.vision_model.encoder.layers.3.self_attn.out_proj.weight\n",
      "vision_tower.vision_model.encoder.layers.3.self_attn.out_proj.bias\n",
      "vision_tower.vision_model.encoder.layers.3.layer_norm1.weight\n",
      "vision_tower.vision_model.encoder.layers.3.layer_norm1.bias\n",
      "vision_tower.vision_model.encoder.layers.3.mlp.fc1.weight\n",
      "vision_tower.vision_model.encoder.layers.3.mlp.fc1.bias\n",
      "vision_tower.vision_model.encoder.layers.3.mlp.fc2.weight\n",
      "vision_tower.vision_model.encoder.layers.3.mlp.fc2.bias\n",
      "vision_tower.vision_model.encoder.layers.3.layer_norm2.weight\n",
      "vision_tower.vision_model.encoder.layers.3.layer_norm2.bias\n",
      "vision_tower.vision_model.encoder.layers.4.self_attn.k_proj.weight\n",
      "vision_tower.vision_model.encoder.layers.4.self_attn.k_proj.bias\n",
      "vision_tower.vision_model.encoder.layers.4.self_attn.v_proj.weight\n",
      "vision_tower.vision_model.encoder.layers.4.self_attn.v_proj.bias\n",
      "vision_tower.vision_model.encoder.layers.4.self_attn.q_proj.weight\n",
      "vision_tower.vision_model.encoder.layers.4.self_attn.q_proj.bias\n",
      "vision_tower.vision_model.encoder.layers.4.self_attn.out_proj.weight\n",
      "vision_tower.vision_model.encoder.layers.4.self_attn.out_proj.bias\n",
      "vision_tower.vision_model.encoder.layers.4.layer_norm1.weight\n",
      "vision_tower.vision_model.encoder.layers.4.layer_norm1.bias\n",
      "vision_tower.vision_model.encoder.layers.4.mlp.fc1.weight\n",
      "vision_tower.vision_model.encoder.layers.4.mlp.fc1.bias\n",
      "vision_tower.vision_model.encoder.layers.4.mlp.fc2.weight\n",
      "vision_tower.vision_model.encoder.layers.4.mlp.fc2.bias\n",
      "vision_tower.vision_model.encoder.layers.4.layer_norm2.weight\n",
      "vision_tower.vision_model.encoder.layers.4.layer_norm2.bias\n",
      "vision_tower.vision_model.encoder.layers.5.self_attn.k_proj.weight\n",
      "vision_tower.vision_model.encoder.layers.5.self_attn.k_proj.bias\n",
      "vision_tower.vision_model.encoder.layers.5.self_attn.v_proj.weight\n",
      "vision_tower.vision_model.encoder.layers.5.self_attn.v_proj.bias\n",
      "vision_tower.vision_model.encoder.layers.5.self_attn.q_proj.weight\n",
      "vision_tower.vision_model.encoder.layers.5.self_attn.q_proj.bias\n",
      "vision_tower.vision_model.encoder.layers.5.self_attn.out_proj.weight\n",
      "vision_tower.vision_model.encoder.layers.5.self_attn.out_proj.bias\n",
      "vision_tower.vision_model.encoder.layers.5.layer_norm1.weight\n",
      "vision_tower.vision_model.encoder.layers.5.layer_norm1.bias\n",
      "vision_tower.vision_model.encoder.layers.5.mlp.fc1.weight\n",
      "vision_tower.vision_model.encoder.layers.5.mlp.fc1.bias\n",
      "vision_tower.vision_model.encoder.layers.5.mlp.fc2.weight\n",
      "vision_tower.vision_model.encoder.layers.5.mlp.fc2.bias\n",
      "vision_tower.vision_model.encoder.layers.5.layer_norm2.weight\n",
      "vision_tower.vision_model.encoder.layers.5.layer_norm2.bias\n",
      "vision_tower.vision_model.encoder.layers.6.self_attn.k_proj.weight\n",
      "vision_tower.vision_model.encoder.layers.6.self_attn.k_proj.bias\n",
      "vision_tower.vision_model.encoder.layers.6.self_attn.v_proj.weight\n",
      "vision_tower.vision_model.encoder.layers.6.self_attn.v_proj.bias\n",
      "vision_tower.vision_model.encoder.layers.6.self_attn.q_proj.weight\n",
      "vision_tower.vision_model.encoder.layers.6.self_attn.q_proj.bias\n",
      "vision_tower.vision_model.encoder.layers.6.self_attn.out_proj.weight\n",
      "vision_tower.vision_model.encoder.layers.6.self_attn.out_proj.bias\n",
      "vision_tower.vision_model.encoder.layers.6.layer_norm1.weight\n",
      "vision_tower.vision_model.encoder.layers.6.layer_norm1.bias\n",
      "vision_tower.vision_model.encoder.layers.6.mlp.fc1.weight\n",
      "vision_tower.vision_model.encoder.layers.6.mlp.fc1.bias\n",
      "vision_tower.vision_model.encoder.layers.6.mlp.fc2.weight\n",
      "vision_tower.vision_model.encoder.layers.6.mlp.fc2.bias\n",
      "vision_tower.vision_model.encoder.layers.6.layer_norm2.weight\n",
      "vision_tower.vision_model.encoder.layers.6.layer_norm2.bias\n",
      "vision_tower.vision_model.encoder.layers.7.self_attn.k_proj.weight\n",
      "vision_tower.vision_model.encoder.layers.7.self_attn.k_proj.bias\n",
      "vision_tower.vision_model.encoder.layers.7.self_attn.v_proj.weight\n",
      "vision_tower.vision_model.encoder.layers.7.self_attn.v_proj.bias\n",
      "vision_tower.vision_model.encoder.layers.7.self_attn.q_proj.weight\n",
      "vision_tower.vision_model.encoder.layers.7.self_attn.q_proj.bias\n",
      "vision_tower.vision_model.encoder.layers.7.self_attn.out_proj.weight\n",
      "vision_tower.vision_model.encoder.layers.7.self_attn.out_proj.bias\n",
      "vision_tower.vision_model.encoder.layers.7.layer_norm1.weight\n",
      "vision_tower.vision_model.encoder.layers.7.layer_norm1.bias\n",
      "vision_tower.vision_model.encoder.layers.7.mlp.fc1.weight\n",
      "vision_tower.vision_model.encoder.layers.7.mlp.fc1.bias\n",
      "vision_tower.vision_model.encoder.layers.7.mlp.fc2.weight\n",
      "vision_tower.vision_model.encoder.layers.7.mlp.fc2.bias\n",
      "vision_tower.vision_model.encoder.layers.7.layer_norm2.weight\n",
      "vision_tower.vision_model.encoder.layers.7.layer_norm2.bias\n",
      "vision_tower.vision_model.encoder.layers.8.self_attn.k_proj.weight\n",
      "vision_tower.vision_model.encoder.layers.8.self_attn.k_proj.bias\n",
      "vision_tower.vision_model.encoder.layers.8.self_attn.v_proj.weight\n",
      "vision_tower.vision_model.encoder.layers.8.self_attn.v_proj.bias\n",
      "vision_tower.vision_model.encoder.layers.8.self_attn.q_proj.weight\n",
      "vision_tower.vision_model.encoder.layers.8.self_attn.q_proj.bias\n",
      "vision_tower.vision_model.encoder.layers.8.self_attn.out_proj.weight\n",
      "vision_tower.vision_model.encoder.layers.8.self_attn.out_proj.bias\n",
      "vision_tower.vision_model.encoder.layers.8.layer_norm1.weight\n",
      "vision_tower.vision_model.encoder.layers.8.layer_norm1.bias\n",
      "vision_tower.vision_model.encoder.layers.8.mlp.fc1.weight\n",
      "vision_tower.vision_model.encoder.layers.8.mlp.fc1.bias\n",
      "vision_tower.vision_model.encoder.layers.8.mlp.fc2.weight\n",
      "vision_tower.vision_model.encoder.layers.8.mlp.fc2.bias\n",
      "vision_tower.vision_model.encoder.layers.8.layer_norm2.weight\n",
      "vision_tower.vision_model.encoder.layers.8.layer_norm2.bias\n",
      "vision_tower.vision_model.encoder.layers.9.self_attn.k_proj.weight\n",
      "vision_tower.vision_model.encoder.layers.9.self_attn.k_proj.bias\n",
      "vision_tower.vision_model.encoder.layers.9.self_attn.v_proj.weight\n",
      "vision_tower.vision_model.encoder.layers.9.self_attn.v_proj.bias\n",
      "vision_tower.vision_model.encoder.layers.9.self_attn.q_proj.weight\n",
      "vision_tower.vision_model.encoder.layers.9.self_attn.q_proj.bias\n",
      "vision_tower.vision_model.encoder.layers.9.self_attn.out_proj.weight\n",
      "vision_tower.vision_model.encoder.layers.9.self_attn.out_proj.bias\n",
      "vision_tower.vision_model.encoder.layers.9.layer_norm1.weight\n",
      "vision_tower.vision_model.encoder.layers.9.layer_norm1.bias\n",
      "vision_tower.vision_model.encoder.layers.9.mlp.fc1.weight\n",
      "vision_tower.vision_model.encoder.layers.9.mlp.fc1.bias\n",
      "vision_tower.vision_model.encoder.layers.9.mlp.fc2.weight\n",
      "vision_tower.vision_model.encoder.layers.9.mlp.fc2.bias\n",
      "vision_tower.vision_model.encoder.layers.9.layer_norm2.weight\n",
      "vision_tower.vision_model.encoder.layers.9.layer_norm2.bias\n",
      "vision_tower.vision_model.encoder.layers.10.self_attn.k_proj.weight\n",
      "vision_tower.vision_model.encoder.layers.10.self_attn.k_proj.bias\n",
      "vision_tower.vision_model.encoder.layers.10.self_attn.v_proj.weight\n",
      "vision_tower.vision_model.encoder.layers.10.self_attn.v_proj.bias\n",
      "vision_tower.vision_model.encoder.layers.10.self_attn.q_proj.weight\n",
      "vision_tower.vision_model.encoder.layers.10.self_attn.q_proj.bias\n",
      "vision_tower.vision_model.encoder.layers.10.self_attn.out_proj.weight\n",
      "vision_tower.vision_model.encoder.layers.10.self_attn.out_proj.bias\n",
      "vision_tower.vision_model.encoder.layers.10.layer_norm1.weight\n",
      "vision_tower.vision_model.encoder.layers.10.layer_norm1.bias\n",
      "vision_tower.vision_model.encoder.layers.10.mlp.fc1.weight\n",
      "vision_tower.vision_model.encoder.layers.10.mlp.fc1.bias\n",
      "vision_tower.vision_model.encoder.layers.10.mlp.fc2.weight\n",
      "vision_tower.vision_model.encoder.layers.10.mlp.fc2.bias\n",
      "vision_tower.vision_model.encoder.layers.10.layer_norm2.weight\n",
      "vision_tower.vision_model.encoder.layers.10.layer_norm2.bias\n",
      "vision_tower.vision_model.encoder.layers.11.self_attn.k_proj.weight\n",
      "vision_tower.vision_model.encoder.layers.11.self_attn.k_proj.bias\n",
      "vision_tower.vision_model.encoder.layers.11.self_attn.v_proj.weight\n",
      "vision_tower.vision_model.encoder.layers.11.self_attn.v_proj.bias\n",
      "vision_tower.vision_model.encoder.layers.11.self_attn.q_proj.weight\n",
      "vision_tower.vision_model.encoder.layers.11.self_attn.q_proj.bias\n",
      "vision_tower.vision_model.encoder.layers.11.self_attn.out_proj.weight\n",
      "vision_tower.vision_model.encoder.layers.11.self_attn.out_proj.bias\n",
      "vision_tower.vision_model.encoder.layers.11.layer_norm1.weight\n",
      "vision_tower.vision_model.encoder.layers.11.layer_norm1.bias\n",
      "vision_tower.vision_model.encoder.layers.11.mlp.fc1.weight\n",
      "vision_tower.vision_model.encoder.layers.11.mlp.fc1.bias\n",
      "vision_tower.vision_model.encoder.layers.11.mlp.fc2.weight\n",
      "vision_tower.vision_model.encoder.layers.11.mlp.fc2.bias\n",
      "vision_tower.vision_model.encoder.layers.11.layer_norm2.weight\n",
      "vision_tower.vision_model.encoder.layers.11.layer_norm2.bias\n",
      "vision_tower.vision_model.encoder.layers.12.self_attn.k_proj.weight\n",
      "vision_tower.vision_model.encoder.layers.12.self_attn.k_proj.bias\n",
      "vision_tower.vision_model.encoder.layers.12.self_attn.v_proj.weight\n",
      "vision_tower.vision_model.encoder.layers.12.self_attn.v_proj.bias\n",
      "vision_tower.vision_model.encoder.layers.12.self_attn.q_proj.weight\n",
      "vision_tower.vision_model.encoder.layers.12.self_attn.q_proj.bias\n",
      "vision_tower.vision_model.encoder.layers.12.self_attn.out_proj.weight\n",
      "vision_tower.vision_model.encoder.layers.12.self_attn.out_proj.bias\n",
      "vision_tower.vision_model.encoder.layers.12.layer_norm1.weight\n",
      "vision_tower.vision_model.encoder.layers.12.layer_norm1.bias\n",
      "vision_tower.vision_model.encoder.layers.12.mlp.fc1.weight\n",
      "vision_tower.vision_model.encoder.layers.12.mlp.fc1.bias\n",
      "vision_tower.vision_model.encoder.layers.12.mlp.fc2.weight\n",
      "vision_tower.vision_model.encoder.layers.12.mlp.fc2.bias\n",
      "vision_tower.vision_model.encoder.layers.12.layer_norm2.weight\n",
      "vision_tower.vision_model.encoder.layers.12.layer_norm2.bias\n",
      "vision_tower.vision_model.encoder.layers.13.self_attn.k_proj.weight\n",
      "vision_tower.vision_model.encoder.layers.13.self_attn.k_proj.bias\n",
      "vision_tower.vision_model.encoder.layers.13.self_attn.v_proj.weight\n",
      "vision_tower.vision_model.encoder.layers.13.self_attn.v_proj.bias\n",
      "vision_tower.vision_model.encoder.layers.13.self_attn.q_proj.weight\n",
      "vision_tower.vision_model.encoder.layers.13.self_attn.q_proj.bias\n",
      "vision_tower.vision_model.encoder.layers.13.self_attn.out_proj.weight\n",
      "vision_tower.vision_model.encoder.layers.13.self_attn.out_proj.bias\n",
      "vision_tower.vision_model.encoder.layers.13.layer_norm1.weight\n",
      "vision_tower.vision_model.encoder.layers.13.layer_norm1.bias\n",
      "vision_tower.vision_model.encoder.layers.13.mlp.fc1.weight\n",
      "vision_tower.vision_model.encoder.layers.13.mlp.fc1.bias\n",
      "vision_tower.vision_model.encoder.layers.13.mlp.fc2.weight\n",
      "vision_tower.vision_model.encoder.layers.13.mlp.fc2.bias\n",
      "vision_tower.vision_model.encoder.layers.13.layer_norm2.weight\n",
      "vision_tower.vision_model.encoder.layers.13.layer_norm2.bias\n",
      "vision_tower.vision_model.encoder.layers.14.self_attn.k_proj.weight\n",
      "vision_tower.vision_model.encoder.layers.14.self_attn.k_proj.bias\n",
      "vision_tower.vision_model.encoder.layers.14.self_attn.v_proj.weight\n",
      "vision_tower.vision_model.encoder.layers.14.self_attn.v_proj.bias\n",
      "vision_tower.vision_model.encoder.layers.14.self_attn.q_proj.weight\n",
      "vision_tower.vision_model.encoder.layers.14.self_attn.q_proj.bias\n",
      "vision_tower.vision_model.encoder.layers.14.self_attn.out_proj.weight\n",
      "vision_tower.vision_model.encoder.layers.14.self_attn.out_proj.bias\n",
      "vision_tower.vision_model.encoder.layers.14.layer_norm1.weight\n",
      "vision_tower.vision_model.encoder.layers.14.layer_norm1.bias\n",
      "vision_tower.vision_model.encoder.layers.14.mlp.fc1.weight\n",
      "vision_tower.vision_model.encoder.layers.14.mlp.fc1.bias\n",
      "vision_tower.vision_model.encoder.layers.14.mlp.fc2.weight\n",
      "vision_tower.vision_model.encoder.layers.14.mlp.fc2.bias\n",
      "vision_tower.vision_model.encoder.layers.14.layer_norm2.weight\n",
      "vision_tower.vision_model.encoder.layers.14.layer_norm2.bias\n",
      "vision_tower.vision_model.encoder.layers.15.self_attn.k_proj.weight\n",
      "vision_tower.vision_model.encoder.layers.15.self_attn.k_proj.bias\n",
      "vision_tower.vision_model.encoder.layers.15.self_attn.v_proj.weight\n",
      "vision_tower.vision_model.encoder.layers.15.self_attn.v_proj.bias\n",
      "vision_tower.vision_model.encoder.layers.15.self_attn.q_proj.weight\n",
      "vision_tower.vision_model.encoder.layers.15.self_attn.q_proj.bias\n",
      "vision_tower.vision_model.encoder.layers.15.self_attn.out_proj.weight\n",
      "vision_tower.vision_model.encoder.layers.15.self_attn.out_proj.bias\n",
      "vision_tower.vision_model.encoder.layers.15.layer_norm1.weight\n",
      "vision_tower.vision_model.encoder.layers.15.layer_norm1.bias\n",
      "vision_tower.vision_model.encoder.layers.15.mlp.fc1.weight\n",
      "vision_tower.vision_model.encoder.layers.15.mlp.fc1.bias\n",
      "vision_tower.vision_model.encoder.layers.15.mlp.fc2.weight\n",
      "vision_tower.vision_model.encoder.layers.15.mlp.fc2.bias\n",
      "vision_tower.vision_model.encoder.layers.15.layer_norm2.weight\n",
      "vision_tower.vision_model.encoder.layers.15.layer_norm2.bias\n",
      "vision_tower.vision_model.encoder.layers.16.self_attn.k_proj.weight\n",
      "vision_tower.vision_model.encoder.layers.16.self_attn.k_proj.bias\n",
      "vision_tower.vision_model.encoder.layers.16.self_attn.v_proj.weight\n",
      "vision_tower.vision_model.encoder.layers.16.self_attn.v_proj.bias\n",
      "vision_tower.vision_model.encoder.layers.16.self_attn.q_proj.weight\n",
      "vision_tower.vision_model.encoder.layers.16.self_attn.q_proj.bias\n",
      "vision_tower.vision_model.encoder.layers.16.self_attn.out_proj.weight\n",
      "vision_tower.vision_model.encoder.layers.16.self_attn.out_proj.bias\n",
      "vision_tower.vision_model.encoder.layers.16.layer_norm1.weight\n",
      "vision_tower.vision_model.encoder.layers.16.layer_norm1.bias\n",
      "vision_tower.vision_model.encoder.layers.16.mlp.fc1.weight\n",
      "vision_tower.vision_model.encoder.layers.16.mlp.fc1.bias\n",
      "vision_tower.vision_model.encoder.layers.16.mlp.fc2.weight\n",
      "vision_tower.vision_model.encoder.layers.16.mlp.fc2.bias\n",
      "vision_tower.vision_model.encoder.layers.16.layer_norm2.weight\n",
      "vision_tower.vision_model.encoder.layers.16.layer_norm2.bias\n",
      "vision_tower.vision_model.encoder.layers.17.self_attn.k_proj.weight\n",
      "vision_tower.vision_model.encoder.layers.17.self_attn.k_proj.bias\n",
      "vision_tower.vision_model.encoder.layers.17.self_attn.v_proj.weight\n",
      "vision_tower.vision_model.encoder.layers.17.self_attn.v_proj.bias\n",
      "vision_tower.vision_model.encoder.layers.17.self_attn.q_proj.weight\n",
      "vision_tower.vision_model.encoder.layers.17.self_attn.q_proj.bias\n",
      "vision_tower.vision_model.encoder.layers.17.self_attn.out_proj.weight\n",
      "vision_tower.vision_model.encoder.layers.17.self_attn.out_proj.bias\n",
      "vision_tower.vision_model.encoder.layers.17.layer_norm1.weight\n",
      "vision_tower.vision_model.encoder.layers.17.layer_norm1.bias\n",
      "vision_tower.vision_model.encoder.layers.17.mlp.fc1.weight\n",
      "vision_tower.vision_model.encoder.layers.17.mlp.fc1.bias\n",
      "vision_tower.vision_model.encoder.layers.17.mlp.fc2.weight\n",
      "vision_tower.vision_model.encoder.layers.17.mlp.fc2.bias\n",
      "vision_tower.vision_model.encoder.layers.17.layer_norm2.weight\n",
      "vision_tower.vision_model.encoder.layers.17.layer_norm2.bias\n",
      "vision_tower.vision_model.encoder.layers.18.self_attn.k_proj.weight\n",
      "vision_tower.vision_model.encoder.layers.18.self_attn.k_proj.bias\n",
      "vision_tower.vision_model.encoder.layers.18.self_attn.v_proj.weight\n",
      "vision_tower.vision_model.encoder.layers.18.self_attn.v_proj.bias\n",
      "vision_tower.vision_model.encoder.layers.18.self_attn.q_proj.weight\n",
      "vision_tower.vision_model.encoder.layers.18.self_attn.q_proj.bias\n",
      "vision_tower.vision_model.encoder.layers.18.self_attn.out_proj.weight\n",
      "vision_tower.vision_model.encoder.layers.18.self_attn.out_proj.bias\n",
      "vision_tower.vision_model.encoder.layers.18.layer_norm1.weight\n",
      "vision_tower.vision_model.encoder.layers.18.layer_norm1.bias\n",
      "vision_tower.vision_model.encoder.layers.18.mlp.fc1.weight\n",
      "vision_tower.vision_model.encoder.layers.18.mlp.fc1.bias\n",
      "vision_tower.vision_model.encoder.layers.18.mlp.fc2.weight\n",
      "vision_tower.vision_model.encoder.layers.18.mlp.fc2.bias\n",
      "vision_tower.vision_model.encoder.layers.18.layer_norm2.weight\n",
      "vision_tower.vision_model.encoder.layers.18.layer_norm2.bias\n",
      "vision_tower.vision_model.encoder.layers.19.self_attn.k_proj.weight\n",
      "vision_tower.vision_model.encoder.layers.19.self_attn.k_proj.bias\n",
      "vision_tower.vision_model.encoder.layers.19.self_attn.v_proj.weight\n",
      "vision_tower.vision_model.encoder.layers.19.self_attn.v_proj.bias\n",
      "vision_tower.vision_model.encoder.layers.19.self_attn.q_proj.weight\n",
      "vision_tower.vision_model.encoder.layers.19.self_attn.q_proj.bias\n",
      "vision_tower.vision_model.encoder.layers.19.self_attn.out_proj.weight\n",
      "vision_tower.vision_model.encoder.layers.19.self_attn.out_proj.bias\n",
      "vision_tower.vision_model.encoder.layers.19.layer_norm1.weight\n",
      "vision_tower.vision_model.encoder.layers.19.layer_norm1.bias\n",
      "vision_tower.vision_model.encoder.layers.19.mlp.fc1.weight\n",
      "vision_tower.vision_model.encoder.layers.19.mlp.fc1.bias\n",
      "vision_tower.vision_model.encoder.layers.19.mlp.fc2.weight\n",
      "vision_tower.vision_model.encoder.layers.19.mlp.fc2.bias\n",
      "vision_tower.vision_model.encoder.layers.19.layer_norm2.weight\n",
      "vision_tower.vision_model.encoder.layers.19.layer_norm2.bias\n",
      "vision_tower.vision_model.encoder.layers.20.self_attn.k_proj.weight\n",
      "vision_tower.vision_model.encoder.layers.20.self_attn.k_proj.bias\n",
      "vision_tower.vision_model.encoder.layers.20.self_attn.v_proj.weight\n",
      "vision_tower.vision_model.encoder.layers.20.self_attn.v_proj.bias\n",
      "vision_tower.vision_model.encoder.layers.20.self_attn.q_proj.weight\n",
      "vision_tower.vision_model.encoder.layers.20.self_attn.q_proj.bias\n",
      "vision_tower.vision_model.encoder.layers.20.self_attn.out_proj.weight\n",
      "vision_tower.vision_model.encoder.layers.20.self_attn.out_proj.bias\n",
      "vision_tower.vision_model.encoder.layers.20.layer_norm1.weight\n",
      "vision_tower.vision_model.encoder.layers.20.layer_norm1.bias\n",
      "vision_tower.vision_model.encoder.layers.20.mlp.fc1.weight\n",
      "vision_tower.vision_model.encoder.layers.20.mlp.fc1.bias\n",
      "vision_tower.vision_model.encoder.layers.20.mlp.fc2.weight\n",
      "vision_tower.vision_model.encoder.layers.20.mlp.fc2.bias\n",
      "vision_tower.vision_model.encoder.layers.20.layer_norm2.weight\n",
      "vision_tower.vision_model.encoder.layers.20.layer_norm2.bias\n",
      "vision_tower.vision_model.encoder.layers.21.self_attn.k_proj.weight\n",
      "vision_tower.vision_model.encoder.layers.21.self_attn.k_proj.bias\n",
      "vision_tower.vision_model.encoder.layers.21.self_attn.v_proj.weight\n",
      "vision_tower.vision_model.encoder.layers.21.self_attn.v_proj.bias\n",
      "vision_tower.vision_model.encoder.layers.21.self_attn.q_proj.weight\n",
      "vision_tower.vision_model.encoder.layers.21.self_attn.q_proj.bias\n",
      "vision_tower.vision_model.encoder.layers.21.self_attn.out_proj.weight\n",
      "vision_tower.vision_model.encoder.layers.21.self_attn.out_proj.bias\n",
      "vision_tower.vision_model.encoder.layers.21.layer_norm1.weight\n",
      "vision_tower.vision_model.encoder.layers.21.layer_norm1.bias\n",
      "vision_tower.vision_model.encoder.layers.21.mlp.fc1.weight\n",
      "vision_tower.vision_model.encoder.layers.21.mlp.fc1.bias\n",
      "vision_tower.vision_model.encoder.layers.21.mlp.fc2.weight\n",
      "vision_tower.vision_model.encoder.layers.21.mlp.fc2.bias\n",
      "vision_tower.vision_model.encoder.layers.21.layer_norm2.weight\n",
      "vision_tower.vision_model.encoder.layers.21.layer_norm2.bias\n",
      "vision_tower.vision_model.encoder.layers.22.self_attn.k_proj.weight\n",
      "vision_tower.vision_model.encoder.layers.22.self_attn.k_proj.bias\n",
      "vision_tower.vision_model.encoder.layers.22.self_attn.v_proj.weight\n",
      "vision_tower.vision_model.encoder.layers.22.self_attn.v_proj.bias\n",
      "vision_tower.vision_model.encoder.layers.22.self_attn.q_proj.weight\n",
      "vision_tower.vision_model.encoder.layers.22.self_attn.q_proj.bias\n",
      "vision_tower.vision_model.encoder.layers.22.self_attn.out_proj.weight\n",
      "vision_tower.vision_model.encoder.layers.22.self_attn.out_proj.bias\n",
      "vision_tower.vision_model.encoder.layers.22.layer_norm1.weight\n",
      "vision_tower.vision_model.encoder.layers.22.layer_norm1.bias\n",
      "vision_tower.vision_model.encoder.layers.22.mlp.fc1.weight\n",
      "vision_tower.vision_model.encoder.layers.22.mlp.fc1.bias\n",
      "vision_tower.vision_model.encoder.layers.22.mlp.fc2.weight\n",
      "vision_tower.vision_model.encoder.layers.22.mlp.fc2.bias\n",
      "vision_tower.vision_model.encoder.layers.22.layer_norm2.weight\n",
      "vision_tower.vision_model.encoder.layers.22.layer_norm2.bias\n",
      "vision_tower.vision_model.encoder.layers.23.self_attn.k_proj.weight\n",
      "vision_tower.vision_model.encoder.layers.23.self_attn.k_proj.bias\n",
      "vision_tower.vision_model.encoder.layers.23.self_attn.v_proj.weight\n",
      "vision_tower.vision_model.encoder.layers.23.self_attn.v_proj.bias\n",
      "vision_tower.vision_model.encoder.layers.23.self_attn.q_proj.weight\n",
      "vision_tower.vision_model.encoder.layers.23.self_attn.q_proj.bias\n",
      "vision_tower.vision_model.encoder.layers.23.self_attn.out_proj.weight\n",
      "vision_tower.vision_model.encoder.layers.23.self_attn.out_proj.bias\n",
      "vision_tower.vision_model.encoder.layers.23.layer_norm1.weight\n",
      "vision_tower.vision_model.encoder.layers.23.layer_norm1.bias\n",
      "vision_tower.vision_model.encoder.layers.23.mlp.fc1.weight\n",
      "vision_tower.vision_model.encoder.layers.23.mlp.fc1.bias\n",
      "vision_tower.vision_model.encoder.layers.23.mlp.fc2.weight\n",
      "vision_tower.vision_model.encoder.layers.23.mlp.fc2.bias\n",
      "vision_tower.vision_model.encoder.layers.23.layer_norm2.weight\n",
      "vision_tower.vision_model.encoder.layers.23.layer_norm2.bias\n",
      "vision_tower.vision_model.post_layernorm.weight\n",
      "vision_tower.vision_model.post_layernorm.bias\n",
      "multi_modal_projector.linear_1.weight\n",
      "multi_modal_projector.linear_1.bias\n",
      "multi_modal_projector.linear_2.weight\n",
      "multi_modal_projector.linear_2.bias\n",
      "language_model.model.embed_tokens.weight\n",
      "language_model.model.layers.0.self_attn.q_proj.weight\n",
      "language_model.model.layers.0.self_attn.q_proj.bias\n",
      "language_model.model.layers.0.self_attn.k_proj.weight\n",
      "language_model.model.layers.0.self_attn.k_proj.bias\n",
      "language_model.model.layers.0.self_attn.v_proj.weight\n",
      "language_model.model.layers.0.self_attn.v_proj.bias\n",
      "language_model.model.layers.0.self_attn.o_proj.weight\n",
      "language_model.model.layers.0.mlp.gate_proj.weight\n",
      "language_model.model.layers.0.mlp.up_proj.weight\n",
      "language_model.model.layers.0.mlp.down_proj.weight\n",
      "language_model.model.layers.0.input_layernorm.weight\n",
      "language_model.model.layers.0.post_attention_layernorm.weight\n",
      "language_model.model.layers.1.self_attn.q_proj.weight\n",
      "language_model.model.layers.1.self_attn.q_proj.bias\n",
      "language_model.model.layers.1.self_attn.k_proj.weight\n",
      "language_model.model.layers.1.self_attn.k_proj.bias\n",
      "language_model.model.layers.1.self_attn.v_proj.weight\n",
      "language_model.model.layers.1.self_attn.v_proj.bias\n",
      "language_model.model.layers.1.self_attn.o_proj.weight\n",
      "language_model.model.layers.1.mlp.gate_proj.weight\n",
      "language_model.model.layers.1.mlp.up_proj.weight\n",
      "language_model.model.layers.1.mlp.down_proj.weight\n",
      "language_model.model.layers.1.input_layernorm.weight\n",
      "language_model.model.layers.1.post_attention_layernorm.weight\n",
      "language_model.model.layers.2.self_attn.q_proj.weight\n",
      "language_model.model.layers.2.self_attn.q_proj.bias\n",
      "language_model.model.layers.2.self_attn.k_proj.weight\n",
      "language_model.model.layers.2.self_attn.k_proj.bias\n",
      "language_model.model.layers.2.self_attn.v_proj.weight\n",
      "language_model.model.layers.2.self_attn.v_proj.bias\n",
      "language_model.model.layers.2.self_attn.o_proj.weight\n",
      "language_model.model.layers.2.mlp.gate_proj.weight\n",
      "language_model.model.layers.2.mlp.up_proj.weight\n",
      "language_model.model.layers.2.mlp.down_proj.weight\n",
      "language_model.model.layers.2.input_layernorm.weight\n",
      "language_model.model.layers.2.post_attention_layernorm.weight\n",
      "language_model.model.layers.3.self_attn.q_proj.weight\n",
      "language_model.model.layers.3.self_attn.q_proj.bias\n",
      "language_model.model.layers.3.self_attn.k_proj.weight\n",
      "language_model.model.layers.3.self_attn.k_proj.bias\n",
      "language_model.model.layers.3.self_attn.v_proj.weight\n",
      "language_model.model.layers.3.self_attn.v_proj.bias\n",
      "language_model.model.layers.3.self_attn.o_proj.weight\n",
      "language_model.model.layers.3.mlp.gate_proj.weight\n",
      "language_model.model.layers.3.mlp.up_proj.weight\n",
      "language_model.model.layers.3.mlp.down_proj.weight\n",
      "language_model.model.layers.3.input_layernorm.weight\n",
      "language_model.model.layers.3.post_attention_layernorm.weight\n",
      "language_model.model.layers.4.self_attn.q_proj.weight\n",
      "language_model.model.layers.4.self_attn.q_proj.bias\n",
      "language_model.model.layers.4.self_attn.k_proj.weight\n",
      "language_model.model.layers.4.self_attn.k_proj.bias\n",
      "language_model.model.layers.4.self_attn.v_proj.weight\n",
      "language_model.model.layers.4.self_attn.v_proj.bias\n",
      "language_model.model.layers.4.self_attn.o_proj.weight\n",
      "language_model.model.layers.4.mlp.gate_proj.weight\n",
      "language_model.model.layers.4.mlp.up_proj.weight\n",
      "language_model.model.layers.4.mlp.down_proj.weight\n",
      "language_model.model.layers.4.input_layernorm.weight\n",
      "language_model.model.layers.4.post_attention_layernorm.weight\n",
      "language_model.model.layers.5.self_attn.q_proj.weight\n",
      "language_model.model.layers.5.self_attn.q_proj.bias\n",
      "language_model.model.layers.5.self_attn.k_proj.weight\n",
      "language_model.model.layers.5.self_attn.k_proj.bias\n",
      "language_model.model.layers.5.self_attn.v_proj.weight\n",
      "language_model.model.layers.5.self_attn.v_proj.bias\n",
      "language_model.model.layers.5.self_attn.o_proj.weight\n",
      "language_model.model.layers.5.mlp.gate_proj.weight\n",
      "language_model.model.layers.5.mlp.up_proj.weight\n",
      "language_model.model.layers.5.mlp.down_proj.weight\n",
      "language_model.model.layers.5.input_layernorm.weight\n",
      "language_model.model.layers.5.post_attention_layernorm.weight\n",
      "language_model.model.layers.6.self_attn.q_proj.weight\n",
      "language_model.model.layers.6.self_attn.q_proj.bias\n",
      "language_model.model.layers.6.self_attn.k_proj.weight\n",
      "language_model.model.layers.6.self_attn.k_proj.bias\n",
      "language_model.model.layers.6.self_attn.v_proj.weight\n",
      "language_model.model.layers.6.self_attn.v_proj.bias\n",
      "language_model.model.layers.6.self_attn.o_proj.weight\n",
      "language_model.model.layers.6.mlp.gate_proj.weight\n",
      "language_model.model.layers.6.mlp.up_proj.weight\n",
      "language_model.model.layers.6.mlp.down_proj.weight\n",
      "language_model.model.layers.6.input_layernorm.weight\n",
      "language_model.model.layers.6.post_attention_layernorm.weight\n",
      "language_model.model.layers.7.self_attn.q_proj.weight\n",
      "language_model.model.layers.7.self_attn.q_proj.bias\n",
      "language_model.model.layers.7.self_attn.k_proj.weight\n",
      "language_model.model.layers.7.self_attn.k_proj.bias\n",
      "language_model.model.layers.7.self_attn.v_proj.weight\n",
      "language_model.model.layers.7.self_attn.v_proj.bias\n",
      "language_model.model.layers.7.self_attn.o_proj.weight\n",
      "language_model.model.layers.7.mlp.gate_proj.weight\n",
      "language_model.model.layers.7.mlp.up_proj.weight\n",
      "language_model.model.layers.7.mlp.down_proj.weight\n",
      "language_model.model.layers.7.input_layernorm.weight\n",
      "language_model.model.layers.7.post_attention_layernorm.weight\n",
      "language_model.model.layers.8.self_attn.q_proj.weight\n",
      "language_model.model.layers.8.self_attn.q_proj.bias\n",
      "language_model.model.layers.8.self_attn.k_proj.weight\n",
      "language_model.model.layers.8.self_attn.k_proj.bias\n",
      "language_model.model.layers.8.self_attn.v_proj.weight\n",
      "language_model.model.layers.8.self_attn.v_proj.bias\n",
      "language_model.model.layers.8.self_attn.o_proj.weight\n",
      "language_model.model.layers.8.mlp.gate_proj.weight\n",
      "language_model.model.layers.8.mlp.up_proj.weight\n",
      "language_model.model.layers.8.mlp.down_proj.weight\n",
      "language_model.model.layers.8.input_layernorm.weight\n",
      "language_model.model.layers.8.post_attention_layernorm.weight\n",
      "language_model.model.layers.9.self_attn.q_proj.weight\n",
      "language_model.model.layers.9.self_attn.q_proj.bias\n",
      "language_model.model.layers.9.self_attn.k_proj.weight\n",
      "language_model.model.layers.9.self_attn.k_proj.bias\n",
      "language_model.model.layers.9.self_attn.v_proj.weight\n",
      "language_model.model.layers.9.self_attn.v_proj.bias\n",
      "language_model.model.layers.9.self_attn.o_proj.weight\n",
      "language_model.model.layers.9.mlp.gate_proj.weight\n",
      "language_model.model.layers.9.mlp.up_proj.weight\n",
      "language_model.model.layers.9.mlp.down_proj.weight\n",
      "language_model.model.layers.9.input_layernorm.weight\n",
      "language_model.model.layers.9.post_attention_layernorm.weight\n",
      "language_model.model.layers.10.self_attn.q_proj.weight\n",
      "language_model.model.layers.10.self_attn.q_proj.bias\n",
      "language_model.model.layers.10.self_attn.k_proj.weight\n",
      "language_model.model.layers.10.self_attn.k_proj.bias\n",
      "language_model.model.layers.10.self_attn.v_proj.weight\n",
      "language_model.model.layers.10.self_attn.v_proj.bias\n",
      "language_model.model.layers.10.self_attn.o_proj.weight\n",
      "language_model.model.layers.10.mlp.gate_proj.weight\n",
      "language_model.model.layers.10.mlp.up_proj.weight\n",
      "language_model.model.layers.10.mlp.down_proj.weight\n",
      "language_model.model.layers.10.input_layernorm.weight\n",
      "language_model.model.layers.10.post_attention_layernorm.weight\n",
      "language_model.model.layers.11.self_attn.q_proj.weight\n",
      "language_model.model.layers.11.self_attn.q_proj.bias\n",
      "language_model.model.layers.11.self_attn.k_proj.weight\n",
      "language_model.model.layers.11.self_attn.k_proj.bias\n",
      "language_model.model.layers.11.self_attn.v_proj.weight\n",
      "language_model.model.layers.11.self_attn.v_proj.bias\n",
      "language_model.model.layers.11.self_attn.o_proj.weight\n",
      "language_model.model.layers.11.mlp.gate_proj.weight\n",
      "language_model.model.layers.11.mlp.up_proj.weight\n",
      "language_model.model.layers.11.mlp.down_proj.weight\n",
      "language_model.model.layers.11.input_layernorm.weight\n",
      "language_model.model.layers.11.post_attention_layernorm.weight\n",
      "language_model.model.layers.12.self_attn.q_proj.weight\n",
      "language_model.model.layers.12.self_attn.q_proj.bias\n",
      "language_model.model.layers.12.self_attn.k_proj.weight\n",
      "language_model.model.layers.12.self_attn.k_proj.bias\n",
      "language_model.model.layers.12.self_attn.v_proj.weight\n",
      "language_model.model.layers.12.self_attn.v_proj.bias\n",
      "language_model.model.layers.12.self_attn.o_proj.weight\n",
      "language_model.model.layers.12.mlp.gate_proj.weight\n",
      "language_model.model.layers.12.mlp.up_proj.weight\n",
      "language_model.model.layers.12.mlp.down_proj.weight\n",
      "language_model.model.layers.12.input_layernorm.weight\n",
      "language_model.model.layers.12.post_attention_layernorm.weight\n",
      "language_model.model.layers.13.self_attn.q_proj.weight\n",
      "language_model.model.layers.13.self_attn.q_proj.bias\n",
      "language_model.model.layers.13.self_attn.k_proj.weight\n",
      "language_model.model.layers.13.self_attn.k_proj.bias\n",
      "language_model.model.layers.13.self_attn.v_proj.weight\n",
      "language_model.model.layers.13.self_attn.v_proj.bias\n",
      "language_model.model.layers.13.self_attn.o_proj.weight\n",
      "language_model.model.layers.13.mlp.gate_proj.weight\n",
      "language_model.model.layers.13.mlp.up_proj.weight\n",
      "language_model.model.layers.13.mlp.down_proj.weight\n",
      "language_model.model.layers.13.input_layernorm.weight\n",
      "language_model.model.layers.13.post_attention_layernorm.weight\n",
      "language_model.model.layers.14.self_attn.q_proj.weight\n",
      "language_model.model.layers.14.self_attn.q_proj.bias\n",
      "language_model.model.layers.14.self_attn.k_proj.weight\n",
      "language_model.model.layers.14.self_attn.k_proj.bias\n",
      "language_model.model.layers.14.self_attn.v_proj.weight\n",
      "language_model.model.layers.14.self_attn.v_proj.bias\n",
      "language_model.model.layers.14.self_attn.o_proj.weight\n",
      "language_model.model.layers.14.mlp.gate_proj.weight\n",
      "language_model.model.layers.14.mlp.up_proj.weight\n",
      "language_model.model.layers.14.mlp.down_proj.weight\n",
      "language_model.model.layers.14.input_layernorm.weight\n",
      "language_model.model.layers.14.post_attention_layernorm.weight\n",
      "language_model.model.layers.15.self_attn.q_proj.weight\n",
      "language_model.model.layers.15.self_attn.q_proj.bias\n",
      "language_model.model.layers.15.self_attn.k_proj.weight\n",
      "language_model.model.layers.15.self_attn.k_proj.bias\n",
      "language_model.model.layers.15.self_attn.v_proj.weight\n",
      "language_model.model.layers.15.self_attn.v_proj.bias\n",
      "language_model.model.layers.15.self_attn.o_proj.weight\n",
      "language_model.model.layers.15.mlp.gate_proj.weight\n",
      "language_model.model.layers.15.mlp.up_proj.weight\n",
      "language_model.model.layers.15.mlp.down_proj.weight\n",
      "language_model.model.layers.15.input_layernorm.weight\n",
      "language_model.model.layers.15.post_attention_layernorm.weight\n",
      "language_model.model.layers.16.self_attn.q_proj.weight\n",
      "language_model.model.layers.16.self_attn.q_proj.bias\n",
      "language_model.model.layers.16.self_attn.k_proj.weight\n",
      "language_model.model.layers.16.self_attn.k_proj.bias\n",
      "language_model.model.layers.16.self_attn.v_proj.weight\n",
      "language_model.model.layers.16.self_attn.v_proj.bias\n",
      "language_model.model.layers.16.self_attn.o_proj.weight\n",
      "language_model.model.layers.16.mlp.gate_proj.weight\n",
      "language_model.model.layers.16.mlp.up_proj.weight\n",
      "language_model.model.layers.16.mlp.down_proj.weight\n",
      "language_model.model.layers.16.input_layernorm.weight\n",
      "language_model.model.layers.16.post_attention_layernorm.weight\n",
      "language_model.model.layers.17.self_attn.q_proj.weight\n",
      "language_model.model.layers.17.self_attn.q_proj.bias\n",
      "language_model.model.layers.17.self_attn.k_proj.weight\n",
      "language_model.model.layers.17.self_attn.k_proj.bias\n",
      "language_model.model.layers.17.self_attn.v_proj.weight\n",
      "language_model.model.layers.17.self_attn.v_proj.bias\n",
      "language_model.model.layers.17.self_attn.o_proj.weight\n",
      "language_model.model.layers.17.mlp.gate_proj.weight\n",
      "language_model.model.layers.17.mlp.up_proj.weight\n",
      "language_model.model.layers.17.mlp.down_proj.weight\n",
      "language_model.model.layers.17.input_layernorm.weight\n",
      "language_model.model.layers.17.post_attention_layernorm.weight\n",
      "language_model.model.layers.18.self_attn.q_proj.weight\n",
      "language_model.model.layers.18.self_attn.q_proj.bias\n",
      "language_model.model.layers.18.self_attn.k_proj.weight\n",
      "language_model.model.layers.18.self_attn.k_proj.bias\n",
      "language_model.model.layers.18.self_attn.v_proj.weight\n",
      "language_model.model.layers.18.self_attn.v_proj.bias\n",
      "language_model.model.layers.18.self_attn.o_proj.weight\n",
      "language_model.model.layers.18.mlp.gate_proj.weight\n",
      "language_model.model.layers.18.mlp.up_proj.weight\n",
      "language_model.model.layers.18.mlp.down_proj.weight\n",
      "language_model.model.layers.18.input_layernorm.weight\n",
      "language_model.model.layers.18.post_attention_layernorm.weight\n",
      "language_model.model.layers.19.self_attn.q_proj.weight\n",
      "language_model.model.layers.19.self_attn.q_proj.bias\n",
      "language_model.model.layers.19.self_attn.k_proj.weight\n",
      "language_model.model.layers.19.self_attn.k_proj.bias\n",
      "language_model.model.layers.19.self_attn.v_proj.weight\n",
      "language_model.model.layers.19.self_attn.v_proj.bias\n",
      "language_model.model.layers.19.self_attn.o_proj.weight\n",
      "language_model.model.layers.19.mlp.gate_proj.weight\n",
      "language_model.model.layers.19.mlp.up_proj.weight\n",
      "language_model.model.layers.19.mlp.down_proj.weight\n",
      "language_model.model.layers.19.input_layernorm.weight\n",
      "language_model.model.layers.19.post_attention_layernorm.weight\n",
      "language_model.model.layers.20.self_attn.q_proj.weight\n",
      "language_model.model.layers.20.self_attn.q_proj.bias\n",
      "language_model.model.layers.20.self_attn.k_proj.weight\n",
      "language_model.model.layers.20.self_attn.k_proj.bias\n",
      "language_model.model.layers.20.self_attn.v_proj.weight\n",
      "language_model.model.layers.20.self_attn.v_proj.bias\n",
      "language_model.model.layers.20.self_attn.o_proj.weight\n",
      "language_model.model.layers.20.mlp.gate_proj.weight\n",
      "language_model.model.layers.20.mlp.up_proj.weight\n",
      "language_model.model.layers.20.mlp.down_proj.weight\n",
      "language_model.model.layers.20.input_layernorm.weight\n",
      "language_model.model.layers.20.post_attention_layernorm.weight\n",
      "language_model.model.layers.21.self_attn.q_proj.weight\n",
      "language_model.model.layers.21.self_attn.q_proj.bias\n",
      "language_model.model.layers.21.self_attn.k_proj.weight\n",
      "language_model.model.layers.21.self_attn.k_proj.bias\n",
      "language_model.model.layers.21.self_attn.v_proj.weight\n",
      "language_model.model.layers.21.self_attn.v_proj.bias\n",
      "language_model.model.layers.21.self_attn.o_proj.weight\n",
      "language_model.model.layers.21.mlp.gate_proj.weight\n",
      "language_model.model.layers.21.mlp.up_proj.weight\n",
      "language_model.model.layers.21.mlp.down_proj.weight\n",
      "language_model.model.layers.21.input_layernorm.weight\n",
      "language_model.model.layers.21.post_attention_layernorm.weight\n",
      "language_model.model.layers.22.self_attn.q_proj.weight\n",
      "language_model.model.layers.22.self_attn.q_proj.bias\n",
      "language_model.model.layers.22.self_attn.k_proj.weight\n",
      "language_model.model.layers.22.self_attn.k_proj.bias\n",
      "language_model.model.layers.22.self_attn.v_proj.weight\n",
      "language_model.model.layers.22.self_attn.v_proj.bias\n",
      "language_model.model.layers.22.self_attn.o_proj.weight\n",
      "language_model.model.layers.22.mlp.gate_proj.weight\n",
      "language_model.model.layers.22.mlp.up_proj.weight\n",
      "language_model.model.layers.22.mlp.down_proj.weight\n",
      "language_model.model.layers.22.input_layernorm.weight\n",
      "language_model.model.layers.22.post_attention_layernorm.weight\n",
      "language_model.model.layers.23.self_attn.q_proj.weight\n",
      "language_model.model.layers.23.self_attn.q_proj.bias\n",
      "language_model.model.layers.23.self_attn.k_proj.weight\n",
      "language_model.model.layers.23.self_attn.k_proj.bias\n",
      "language_model.model.layers.23.self_attn.v_proj.weight\n",
      "language_model.model.layers.23.self_attn.v_proj.bias\n",
      "language_model.model.layers.23.self_attn.o_proj.weight\n",
      "language_model.model.layers.23.mlp.gate_proj.weight\n",
      "language_model.model.layers.23.mlp.up_proj.weight\n",
      "language_model.model.layers.23.mlp.down_proj.weight\n",
      "language_model.model.layers.23.input_layernorm.weight\n",
      "language_model.model.layers.23.post_attention_layernorm.weight\n",
      "language_model.model.norm.weight\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-14T07:58:36.401127Z",
     "start_time": "2024-07-14T07:58:36.391400Z"
    }
   },
   "cell_type": "code",
   "source": [
    "llava_model.config.use_cache=False\n",
    "llava_model.config"
   ],
   "id": "6cca6303c65d35dd",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlavaConfig {\n",
       "  \"_name_or_path\": \"model/llava\",\n",
       "  \"architectures\": [\n",
       "    \"LlavaForConditionalGeneration\"\n",
       "  ],\n",
       "  \"ignore_index\": -100,\n",
       "  \"image_token_index\": 151646,\n",
       "  \"model_type\": \"llava\",\n",
       "  \"pad_token_id\": 151643,\n",
       "  \"projector_hidden_act\": \"gelu\",\n",
       "  \"text_config\": {\n",
       "    \"_name_or_path\": \"model/qwen1.5_0.5b\",\n",
       "    \"architectures\": [\n",
       "      \"Qwen2ForCausalLM\"\n",
       "    ],\n",
       "    \"bos_token_id\": 151643,\n",
       "    \"eos_token_id\": 151645,\n",
       "    \"hidden_size\": 1024,\n",
       "    \"intermediate_size\": 2816,\n",
       "    \"max_position_embeddings\": 32768,\n",
       "    \"max_window_layers\": 21,\n",
       "    \"model_type\": \"qwen2\",\n",
       "    \"num_attention_heads\": 16,\n",
       "    \"num_hidden_layers\": 24,\n",
       "    \"num_key_value_heads\": 16,\n",
       "    \"rope_theta\": 1000000.0,\n",
       "    \"sliding_window\": 32768,\n",
       "    \"tie_word_embeddings\": true,\n",
       "    \"torch_dtype\": \"bfloat16\",\n",
       "    \"use_sliding_window\": false,\n",
       "    \"vocab_size\": 151936\n",
       "  },\n",
       "  \"torch_dtype\": \"float32\",\n",
       "  \"transformers_version\": \"4.42.4\",\n",
       "  \"use_cache\": false,\n",
       "  \"vision_config\": {\n",
       "    \"dropout\": 0.0,\n",
       "    \"hidden_size\": 1024,\n",
       "    \"image_size\": 336,\n",
       "    \"intermediate_size\": 4096,\n",
       "    \"model_type\": \"clip_vision_model\",\n",
       "    \"num_attention_heads\": 16,\n",
       "    \"num_hidden_layers\": 24,\n",
       "    \"patch_size\": 14,\n",
       "    \"projection_dim\": 768\n",
       "  },\n",
       "  \"vision_feature_layer\": -2,\n",
       "  \"vision_feature_select_strategy\": \"default\"\n",
       "}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
