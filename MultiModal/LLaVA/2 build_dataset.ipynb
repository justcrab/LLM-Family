{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "import json\n",
    "import os.path as osp"
   ],
   "id": "initial_id",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "1 LlavaDataset\n",
    "建议在Dataset里面完成tokenize，这样在Collator中就可以直接找到batch中的最大序列长度"
   ],
   "id": "5a64151c769e4757"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-13T05:13:21.064838Z",
     "start_time": "2024-07-13T05:13:21.044733Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class LlavaDataset(Dataset):\n",
    "    def __init__(self, processor, data_path, image_dir, ignore_index):\n",
    "        super().__init__()\n",
    "        self.image_dir = image_dir\n",
    "        self.processor = processor\n",
    "        self.ignore_index = ignore_index\n",
    "        self.data_list = json.load(open(data_path, \"r\"))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        data = self.data_list[item]\n",
    "        conversations = data[\"conversations\"]\n",
    "        image_path = osp.join(self.image_dir, data[\"image\"])\n",
    "        human = conversations[0][\"value\"]\n",
    "        gpt = conversations[1][\"value\"]\n",
    "        human_input_ids = self.processor.tokenizer(human)[\"input_ids\"]\n",
    "        gpt_input_ids = self.processor.tokenizer(gpt)[\"input_ids\"]\n",
    "        input_ids = human_input_ids + gpt_input_ids\n",
    "        attention_mask = [1] * len(input_ids)\n",
    "        labels = [self.ignore_index]*len(human_input_ids)  + gpt_input_ids\n",
    "        pixel_values = self.processor.image_processor(Image.open(image_path))[\"pixel_values\"][0]\n",
    "        return (input_ids, attention_mask, labels, pixel_values)"
   ],
   "id": "965333a1198cf62b",
   "execution_count": 30,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-13T05:13:23.333094Z",
     "start_time": "2024-07-13T05:13:22.712672Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import LlavaProcessor\n",
    "\n",
    "data_path, image_dir = \"data/try.json\", \"images\"\n",
    "processor = LlavaProcessor.from_pretrained(\"model/llava\")\n",
    "dataset = LlavaDataset(processor, data_path, image_dir, -100)\n",
    "print(dataset[0])"
   ],
   "id": "5012efcd007cac35",
   "execution_count": 31,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "2 LlaVAForTrainCollator",
   "id": "c041c2b124ba896b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-13T05:13:45.928814Z",
     "start_time": "2024-07-13T05:13:45.913681Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from PIL import Image\n",
    "\n",
    "class LlaVAForTrainCollator:\n",
    "    def __init__(self, max_length, ignore_index):\n",
    "        self.max_length = max_length\n",
    "        self.ignore_index = ignore_index\n",
    "        self.pad_token_id = processor.tokenizer.pad_token_id\n",
    "        \n",
    "    def __call__(self, batch):\n",
    "        batch_max_length = [len(sampler[0]) for sampler in batch]\n",
    "        final_max_length = min(max(batch_max_length), self.max_length)\n",
    "        batch_input_ids, batch_attention_mask, batch_labels, batch_pixel_values = [], [], [], []\n",
    "        for sampler in batch:\n",
    "            # 1 input_ids, attention_mask, labels, pixel_values\n",
    "            input_ids, attention_mask, labels, pixel_values = sampler\n",
    "            # 2 padding\n",
    "            padding_len = final_max_length - len(input_ids)\n",
    "            input_ids = [self.pad_token_id] * padding_len + input_ids\n",
    "            attention_mask = [0] * padding_len + attention_mask\n",
    "            labels = [self.ignore_index] * padding_len  + labels\n",
    "            # 3 truncate\n",
    "            input_ids = input_ids[:self.max_length]\n",
    "            attention_mask = attention_mask[:self.max_length]\n",
    "            labels = labels[:self.max_length]\n",
    "            # 4 batch\n",
    "            batch_input_ids.append(input_ids)\n",
    "            batch_attention_mask.append(attention_mask)\n",
    "            batch_labels.append(labels)\n",
    "            batch_pixel_values.append(pixel_values)\n",
    "        # 5 tensor\n",
    "        batch_input_ids = torch.tensor(batch_input_ids, dtype=torch.long)\n",
    "        batch_attention_mask = torch.tensor(batch_attention_mask, dtype=torch.long)\n",
    "        batch_labels = torch.tensor(batch_labels, dtype=torch.long)\n",
    "        batch_pixel_values = torch.tensor(batch_pixel_values, dtype=torch.float)\n",
    "            \n",
    "        return {\n",
    "            \"input_ids\": batch_input_ids, \"attention_mask\": batch_attention_mask, \n",
    "            \"labels\": batch_labels, \"pixel_values\": batch_pixel_values\n",
    "        }"
   ],
   "id": "f2ba902594c75451",
   "execution_count": 32,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-13T05:13:47.943508Z",
     "start_time": "2024-07-13T05:13:47.823091Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "collator = LlaVAForTrainCollator(1024, -100)\n",
    "output = collator([dataset[0], dataset[1]])\n",
    "output"
   ],
   "id": "88cfcac8aadeedad",
   "execution_count": 33,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
